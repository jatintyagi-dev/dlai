# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_learner.ipynb.

# %% auto 0
__all__ = ['def_device', 'DataLoaders', 'inplace', 'collate_dict', 'cb', 'rcb', 'to_device', 'CancelFitException',
           'CancelEpochException', 'CancelBatchException', 'cb_dec', 'learner', 'TrainLearner', 'momentumLearner']

# %% ../nbs/03_learner.ipynb 2
import math,torch,matplotlib.pyplot as plt
import fastcore.all as fc
from collections.abc import Mapping
from operator import attrgetter
from functools import partial
from copy import copy

from torch import optim
import torch.nn.functional as F
from torch.utils.data import default_collate

# from miniai.conv import *

from fastprogress import progress_bar,master_bar
from operator import itemgetter

# %% ../nbs/03_learner.ipynb 9
class DataLoaders:
    def __init__(self, *dls): self.train,self.valid = dls[:2]

    @classmethod
    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):
        f = collate_dict(dd['train'])
        return cls(*get_dls(*dd.values(), bs=batch_size, collate_fn=f, **kwargs))

# %% ../nbs/03_learner.ipynb 12
def inplace(f):
    def _f(b):
        f(b)
        return b
    return _f

# %% ../nbs/03_learner.ipynb 13
def collate_dict(ds):
    get = itemgetter(*ds.features)
    def _f(b): return get(default_collate(b))
    return _f

# %% ../nbs/03_learner.ipynb 21
class cb:
    order = 0
        

# %% ../nbs/03_learner.ipynb 27
def rcb(cbs, method_name, learn):
    for cb in sorted(cbs, key= attrgetter("order")):
        method =  getattr(cb, method_name, None )
        if method : return method(learn)
        

# %% ../nbs/03_learner.ipynb 31
def_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'

def to_device(x, device=def_device):
    if isinstance(x, torch.Tensor): return x.to(device)
    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}
    return type(x)(to_device(o, device) for o in x)

# %% ../nbs/03_learner.ipynb 44
class CancelFitException:pass
class CancelEpochException:pass
class CancelBatchException:pass

# %% ../nbs/03_learner.ipynb 46
class cb_dec:
    def __init__(self, name):
        fc.store_attr()
        
    def __call__(self, f):
        def _f(o,*args,**kwargs):
            try:
                o.callback(f"before_{self.name}")
                f(o,*args, **kwargs)
                o.callback(f"after_{self.name}")
            except: globals()[f"Cancel{self.name.title()}Exception"]()
        return _f 
            
            

# %% ../nbs/03_learner.ipynb 47
class learner:
    
    def __init__(self, model, dls,loss_func, lr,cbs=[] ,opt_func=optim.SGD):fc.store_attr()
    
    @cb_dec("batch")
    def one_batch(self):
        x,y = to_device(self.b)
        
        self.preds = self.model(x)
        self.loss = self.loss_func(self.preds, y)
        if self.model.training: 
            self.loss.backward()
            self.opt.step()
            self.opt.zero_grad()
#         else :
#             with torch.no_grad():
#                 self.preds = self.model(b[0])
            
        
    @cb_dec("epoch")
    def one_epoch(self, train):
        self.model.training = train
        dl = self.dls.train if train else self.dls.valid
        
        for self.b in dl:self.one_batch()
    
    @cb_dec("fit")
    def fit(self, epochs):
        self.model = self.model.to(def_device)
        self.opt = self.opt_func(self.model.parameters(), self.lr)
        
        for e in range(epochs):
            self.one_epoch(True)
            with torch.no_grad(): self.one_epoch(False)

            print(f"epoch {e} loss :{self.loss}")
                
    def callback(self, nm): rcb(self.cbs, nm, self)
        

# %% ../nbs/03_learner.ipynb 52
class TrainLearner(learner):
    def predict(self): self.preds = self.model(self.batch[0])
    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])
    def backward(self): self.loss.backward()
    def step(self): self.opt.step()
    def zero_grad(self): self.opt.zero_grad()

# %% ../nbs/03_learner.ipynb 55
class momentumLearner(learner):
    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):
        self.mom = mom
        super().__init__(model, dls, loss_func, lr, cbs, opt_func)
        
    def step(self): self.opt.step()
        
    def zero_grad(self): 
        with torch.no_grad():
            for p in self.model.parameters(): p.grad *= self.mom
